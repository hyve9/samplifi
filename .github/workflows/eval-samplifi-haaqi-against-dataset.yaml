name: Run Samplifi on Dataset

on:
  workflow_dispatch:
    inputs:
      dataset:
        description: 'Dataset to download and process'
        required: true
      sample_size:
        description: 'Number of samples to download'
        required: false
        default: '250'

jobs:
  run-samplifi:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v2

    # Running out of disk space on git runner
    - name: Free Disk Space (Ubuntu)
      uses: jlumbroso/free-disk-space@main
      with:
        # this might remove tools that are actually needed,
        # when set to "true" but frees about 6 GB
        tool-cache: true

    - name: Setup Miniconda
      uses: conda-incubator/setup-miniconda@v2
      with:
        auto-update-conda: true
        python-version: '3.9'  # Use Python 3.9

    - name: Create and activate Conda environment
      run: |
        echo "source $(conda info --base)/etc/profile.d/conda.sh" >> ~/.bashrc
        source ~/.bashrc
        conda env create -f environment.yml
        conda activate samplifi
        python -m pip install tensorflowjs ddsp --no-deps

    - name: Download dataset
      timeout-minutes: 180
      run: |
        source $(conda info --base)/etc/profile.d/conda.sh
        conda activate samplifi
        python download-mir-dataset.py --dataset "${{ github.event.inputs.dataset }}" --sample-size "${{ github.event.inputs.sample_size }}"

    - name: Run Samplifi
      run: |
        source $(conda info --base)/etc/profile.d/conda.sh
        conda activate samplifi
        python run-samplifi.py --dataset ${{ github.event.inputs.dataset }}

    - name: Upload CSV as artifact
      uses: actions/upload-artifact@v2
      with:
        name: samplifi-output
        path: haaqi_scores.csv # Replace with the path to your CSV file
